Tasks To Be Performed:
 1. Use the previous assignment‚Äôs deployment
 2. Deploy any 2 containers in a overlay network
 3. Try pinging each of the containers from within the containers

---

Here's a **detailed solution** for your assignment based on Docker Swarm:

---

## üßæ **Assignment Tasks Overview**

### ‚úÖ Task 1: Use the previous assignment‚Äôs deployment

We assume you already have a Docker Swarm cluster running with an Apache service deployed.

### ‚úÖ Task 2: Deploy any 2 containers in an **overlay network**

### ‚úÖ Task 3: **Ping each container from the other** inside the containers

---

## üõ†Ô∏è **Step-by-Step Detailed Solution**

---

### ‚úÖ **Step 1: Ensure Docker Swarm Is Initialized**

If you haven‚Äôt already, initialize Swarm on the manager:

```bash
docker swarm init --advertise-addr <MANAGER-IP>
```

---

### ‚úÖ **Step 2: Create an Overlay Network**

Overlay networks allow containers across multiple nodes to communicate securely.

```bash
docker network create \
  --driver overlay \
  --attachable \
  my_overlay_net
```

* `--driver overlay`: Creates a multi-host network
* `--attachable`: Allows containers to be manually connected to the network (not just services)

---

### ‚úÖ **Step 3: Deploy Two Containers (as Services) in the Overlay Network**

We‚Äôll deploy 2 simple containers (using Alpine for simplicity), and attach them to the `my_overlay_net`.

#### üü¢ Container 1:

```bash
docker service create \
  --name alpine1 \
  --network my_overlay_net \
  --replicas 1 \
  alpine \
  sleep 10000
```

#### üü¢ Container 2:

```bash
docker service create \
  --name alpine2 \
  --network my_overlay_net \
  --replicas 1 \
  alpine \
  sleep 10000
```

> These services just run a long `sleep` command to keep them alive for testing.

---

### ‚úÖ **Step 4: Identify the Containers**

Get the container IDs (you may run this on each node if needed):

```bash
docker ps
```

Look for containers named like `alpine1.XXXX` and `alpine2.XXXX`.

---

### ‚úÖ **Step 5: Ping Between Containers**

To ping one container from another:

1. **Get the container shell (into alpine1):**

```bash
docker exec -it <alpine1_container_id> sh
```

2. **Ping the other container (alpine2):**

You can ping by container **service name**, thanks to built-in DNS in the overlay network:

```sh
ping alpine2
```

You should see successful ping replies.

3. **Now do the reverse**:

   * `exit` the current container
   * `docker exec -it <alpine2_container_id> sh`
   * `ping alpine1`

---

### ‚úÖ **Expected Output**

You should see something like this when pinging:

```
PING alpine2 (10.0.0.5): 56 data bytes
64 bytes from 10.0.0.5: seq=0 ttl=64 time=0.124 ms
64 bytes from 10.0.0.5: seq=1 ttl=64 time=0.121 ms
...
```

This confirms the overlay network is working and containers can talk to each other across nodes.

---

## üßº Optional Cleanup

To remove services and network:

```bash
docker service rm alpine1 alpine2
docker network rm my_overlay_net
```

---

## ‚úÖ Summary of Commands

```bash
docker network create --driver overlay --attachable my_overlay_net

docker service create --name alpine1 --network my_overlay_net --replicas 1 alpine sleep 10000

docker service create --name alpine2 --network my_overlay_net --replicas 1 alpine sleep 10000

# Get container ID and ping inside
docker exec -it <container_id> sh
ping alpine2
```

---

Yes, you should **open port 8080** (or any other relevant port) on all the nodes **if you're using it for any service** that needs to be accessed by other nodes in the Docker Swarm cluster, such as a web service running on the manager node.

However, based on your Docker Swarm setup, if you are only concerned with the communication between Docker Swarm nodes (manager and worker), the specific ports that need to be open are the **Swarm-related ports** (2377, 7946, 4789) as mentioned earlier. Port **8080** would only need to be open if you're running a service (like a web application or monitoring tool) on port 8080 that needs to be accessed by other nodes or external clients.

### To Summarize:

1. **For Docker Swarm Communication**:

   * **Ports 2377**, **7946**, and **4789** should be open between all nodes (manager and worker).
   * These are the ports that are necessary for the internal functioning of Docker Swarm, and they need to be open on **all nodes** in the swarm.

2. **For External Service (e.g., Web App on Port 8080)**:

   * If your manager node (IP `50.17.38.246`) is hosting a service on **port 8080** (such as a web app or monitoring dashboard), and if you need access to that service from **other nodes** or **external clients**, then you need to open port **8080** on the manager node in its **security group** and on any firewall on that node.

3. **On Worker Nodes**:

   * You only need to open ports **2377**, **7946**, and **4789** for Docker Swarm communication.
   * You **don't need to open port 8080 on the worker nodes** unless those nodes are accessing services running on the manager node that are exposed on that port.

### Opening Port 8080 on the Manager Node (for External Access):

If you want to allow access to the manager's service running on port 8080 from external clients or other nodes, do the following:

#### AWS Security Group:

1. Go to the **EC2 Dashboard** and select the **Security Group** attached to the manager node.
2. Add an **Inbound rule** for **TCP port 8080**.
3. Specify the allowed source (e.g., `0.0.0.0/0` for any external access or IP range to restrict access).

#### Firewall on the Manager Node:

If the manager is running a firewall like `ufw`, you can allow traffic on port 8080:

```bash
# Open port 8080 for external access
sudo ufw allow 8080/tcp
```

---
